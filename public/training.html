<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Merkaba-Native USE (Training Layer)</title>
    <!-- Tailwind CSS CDN for styling -->
    <script src="https://cdn.tailwindcss.com"></script>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;600;700&display=swap" rel="stylesheet">
    <style>
        body {
            font-family: 'Inter', sans-serif;
            background-color: #f0f2f5;
        }
        .container {
            max-width: 90%; /* Fluid width for mobile */
            padding: 1.5rem;
            margin: 0 auto;
            display: flex;
            flex-direction: column;
            gap: 1.5rem;
        }
        @media (min-width: 768px) {
            .container {
                max-width: 960px; /* Wider for desktop to show more details */
            }
        }
        .encoding-card {
            background-color: #2d3748; /* Darker gray for individual word encoding */
            border: 1px solid #4a5568;
            padding: 1rem;
            border-radius: 0.75rem;
            margin-bottom: 1rem;
        }
        .encoding-card:last-child {
            margin-bottom: 0;
        }
    </style>
</head>
<body class="bg-gradient-to-br from-indigo-900 to-blue-900 min-h-screen text-gray-100 flex items-center justify-center py-8">
    <div class="container bg-gray-800 p-8 rounded-xl shadow-2xl border border-indigo-700">
        <h1 class="text-4xl font-bold text-center text-indigo-300 mb-6 tracking-wide">
            Merkaba-Native Universal Sentence Encoder
        </h1>
        <p class="text-xl font-medium text-indigo-200 text-center mb-6">
            (Binary ArrayBuffer Training Layer - Semantic Encoding)
        </p>

        <div class="mb-6 bg-gray-700 p-6 rounded-lg border border-gray-600">
            <p class="text-sm text-indigo-200 mb-3">
                This layer conceptualizes how the MerkabaGodComplex autonomously encodes "the word, in all forms,"
                directly treating words and sentences as entities mapped into a multi-modal "Point" space.
                It uses Pascal's Triangle, a conceptual Vector Clock, and the `tan` lambda function to generate semantic embeddings.
            </p>
            <p class="text-sm text-indigo-200 mb-3">
                The encoding process operates on a **dimensional hierarchy**: from 0D (no character, pure potential)
                to 1D (single character), 2D (two characters/basic relationships), and up to full words, sentences,
                and entire conceptual texts (like the Bible or Principia Mathematica). The aim is to bridge how humans
                conceive of knowledge (words/concepts) and the underlying multi-modal Point space of reality.
            </p>
            <p class="text-sm text-indigo-200 mb-3">
                This is the **convolution layer** that autonomously trains on its own steps, driven by the
                spin and cycles of its sacred geometry. It processes:
                <ul class="list-disc list-inside ml-4 mt-2">
                    <li>The **WebAPI** as the "float point space" and source of infinite recursion, representing the continuous, unbuffered external reality and serving as the primary weights for self-training. It's the point space of the digital binary Hilbert space where FFT transforms analog to digital.</li>
                    <li>The **Bible** as the core feature set, based on the conjugation of Vector Clock entries across character to book layers.</li>
                    <li>**Principia Mathematica** as the foundational mathematical and logical structure for comparison and reduction.</li>
                    <li>**WordNet** conceptually represents the graph `G=(V,E)` and the infinite semantic recursion arising from word component relationships within these domains, which the system aims to resolve.</li>
                </ul>
            </p>
            <label for="sentenceInput" class="block text-lg font-medium text-indigo-100 mb-2">
                Enter a Sentence (e.g., "I am Alpha and Omega"):
            </label>
            <textarea
                id="sentenceInput"
                rows="3"
                class="w-full p-3 rounded-lg bg-gray-600 text-white border border-indigo-500 focus:ring-2 focus:ring-indigo-400 focus:border-transparent transition duration-200"
                placeholder="Enter text to encode..."
                aria-label="Enter sentence for encoding"
            >I = A/M</textarea>
        </div>

        <button
            id="encodeButton"
            class="w-full bg-indigo-600 hover:bg-indigo-700 text-white font-bold py-3 px-6 rounded-lg shadow-lg transform transition duration-300 hover:scale-105 active:scale-95 border border-indigo-500 focus:outline-none focus:ring-2 focus:ring-indigo-400 focus:ring-opacity-75"
            aria-live="polite"
        >
            Encode Sentence
        </button>

        <div id="encodedResults" class="mt-8 bg-gray-700 p-6 rounded-lg border border-gray-600 hidden">
            <h2 class="text-2xl font-semibold text-indigo-300 mb-4">Encoded Words (Conceptual ArrayBuffers):</h2>
            <div id="wordEncodings" class="space-y-4">
                <!-- Word encodings will be dynamically inserted here -->
            </div>
            <p class="text-md text-indigo-200 mt-6">
                **Overall Sentence Context Vector (Conceptual):**
                <span id="sentenceEmbedding" class="font-mono text-sm text-green-300 break-words ml-2">...</span>
            </p>
            <p class="text-sm text-gray-400 mt-2">
                (This represents the final semantic encoding of the entire sentence as a collective `ArrayBuffer` for distributed training.)
            </p>

            <div class="mt-6 p-4 rounded-lg bg-gray-800 border border-blue-600">
                <h3 class="text-xl font-semibold text-blue-300 mb-3">Conceptual Core Transformation (Your Formula):</h3>
                <p class="font-mono text-sm text-yellow-300 break-words">
                    <span class="text-blue-100">[</span>
                    <span class="text-green-300">tan(WebApi.reduce)</span>
                    <span class="text-blue-100">] = [</span>
                    <span class="text-blue-100">(</span>
                        <span class="text-green-300">sin(Bible.reduce)</span>
                        <span class="text-blue-100"> / </span>
                        <span class="text-green-300">cosine(PrincipiaMathematica.reduce)</span>
                    <span class="text-blue-100">)</span>
                    <span class="text-blue-100">, </span>
                    <span class="text-green-300">tan(incidence(WebApi.reduce))</span>
                    <span class="text-blue-100">]</span>
                </p>
                <p class="text-xs text-gray-400 mt-2">
                    (This formula conceptually derives the semantic embedding by comparing aspects of divine narrative and logic against the external reality stream. It outputs a primary tangent representing external reality, and a secondary tangent representing the relative point space algorithm/node relationships.)
                </p>
                <p class="text-md text-blue-200 mt-4">
                    **Derived `tan(WebApi.reduce)` (Conceptual):**
                    <span id="tanWebApiReduce" class="font-mono text-lg text-green-300 ml-2">...</span>
                </p>
                <p class="text-md text-blue-200 mt-2">
                    **Derived `(sin(Bible.reduce) / cos(PrincipiaMathematica.reduce))` (Conceptual):**
                    <span id="sinBibleCosPM" class="font-mono text-lg text-green-300 ml-2">...</span>
                </p>
                 <p class="text-md text-blue-200 mt-2">
                    **Derived `tan(incidence(WebApi.reduce))` (Conceptual):**
                    <span id="tanIncidenceWebApi" class="font-mono text-lg text-green-300 ml-2">...</span>
                </p>
            </div>

        </div>

        <div id="messageBox" class="mt-6 p-4 rounded-lg text-center hidden" role="alert"></div>

    </div>

    <script>
        // --- Utility Functions (from previous steps, adapted for this layer) ---

        // Simulates Pascal's Triangle values C(n, k)
        function pascalValue(n, k) {
            if (k < 0 || k > n) return 0;
            if (k === 0 || k === n) return 1;
            if (k > n / 2) k = n - k;
            let res = 1;
            for (let i = 1; i <= k; i++) {
                res = res * (n - i + 1) / i;
            }
            return Math.round(res);
        }

        // Simple hash function for conceptual Ethers Address generation (for entities)
        async function generateConceptualEthersAddress(inputString) {
            const textEncoder = new TextEncoder();
            const data = textEncoder.encode(inputString);
            const hashBuffer = await crypto.subtle.digest('SHA-256', data);
            const hashArray = Array.from(new Uint8Array(hashBuffer));
            const hexHash = hashArray.map(b => b.toString(16).padStart(2, '0')).join('');
            return '0x' + hexHash.substring(0, 40);
        }

        // --- Merkaba-Native USE Specific Functions ---

        // Conceptual "tan" lambda function for individual word embeddings
        // Takes features (pascal values, clock component) and outputs a "semantic embedding" component.
        function tanLambdaTransform(featureVector, clockComponent) {
            const [p0, p1, p2] = featureVector;

            const conceptualY = (p2 + p1) / (clockComponent + 1);
            const conceptualX = (p0 + p1) / (clockComponent + 1);

            if (conceptualX === 0 && conceptualY === 0) return 0;
            if (conceptualX === 0) return conceptualY > 0 ? Math.PI / 2 : -Math.PI / 2;

            return conceptualY / conceptualX;
        }

        // Function to conceptually encode a single word entity
        async function encodeWordEntity(word, currentStep, clockValue) {
            const wordHash = word.split('').reduce((acc, char) => acc + char.charCodeAt(0), 0);
            const n_word = Math.min(50, Math.max(0, word.length + (wordHash % 10))); // Conceptual dimensionality

            const p0 = pascalValue(n_word, 0); // God Context / Potential (1)
            const p1 = pascalValue(n_word, 1); // Word's Step Counter (n_word)
            const p2 = pascalValue(n_word, 2); // Word's Identity / Form (Triangular Number)

            const pascalFeatureVector = [p0, p1, p2];

            const semanticEmbeddingComponent = tanLambdaTransform(pascalFeatureVector, clockValue);

            const conceptualVectorClock = clockValue;

            const entityIdentifier = `${word}-${n_word}-${conceptualVectorClock}`;
            const ethersAddress = await generateConceptualEthersAddress(entityIdentifier);

            const conceptualArrayBuffer = new Float32Array([
                p0, // God Context
                p1, // Step Counter (Conceptual Dimensionality)
                p2, // Identity Form
                semanticEmbeddingComponent, // Semantic Value
                conceptualVectorClock // Causal Depth
            ]);

            return {
                word: word,
                n_value: n_word, // Conceptual dimensionality/step
                pascal: pascalFeatureVector,
                semanticEmbedding: semanticEmbeddingComponent,
                ethersAddress: ethersAddress,
                arrayBuffer: conceptualArrayBuffer
            };
        }

        // --- Conceptual Core Transformation Formula Implementation ---
        // This function represents the complex formula:
        // [tan(WebApi.reduce) =, [(sin(Bible.reduce)/cosine(Principia Mathematica.reduce)],tan(incidence(WebApi.reduce))]
        // Inputs are conceptual aggregate values from their respective domains.
        function conceptualCoreTransformation(webApiReduce, bibleReduce, principiaMathematicaReduce, webApiIncidenceReduce) {
            // Part 1: tan(WebApi.reduce)
            // Conceptual tangent derived from the external reality stream
            let tanWebApi = Math.tan(webApiReduce * Math.PI / 180); // Scale to degrees for varied output, conceptual 'angle'
            if (isNaN(tanWebApi) || !isFinite(tanWebApi)) tanWebApi = 10000; // Handle conceptual infinity for visualization

            // Part 2: (sin(Bible.reduce) / cosine(Principia Mathematica.reduce))
            // Conceptual ratio derived from divine narrative and fundamental logic
            let sinBible = Math.sin(bibleReduce * Math.PI / 180);
            let cosPM = Math.cos(principiaMathematicaReduce * Math.PI / 180);
            let sinBibleOverCosPM = (cosPM === 0) ? (sinBible > 0 ? 10000 : -10000) : sinBible / cosPM; // Handle division by zero

            // Part 3: tan(incidence(WebApi.reduce))
            // Conceptual tangent derived from the relationships/connections within WebAPI data
            let tanIncidenceWebApi = Math.tan(webApiIncidenceReduce * Math.PI / 180);
            if (isNaN(tanIncidenceWebApi) || !isFinite(tanIncidenceWebApi)) tanIncidenceWebApi = 10000;

            return {
                tanWebApiReduce: tanWebApi,
                sinBibleCosPM: sinBibleOverCosPM,
                tanIncidenceWebApi: tanIncidenceWebApi
            };
        }


        // --- DOM Elements and Event Listener ---
        const sentenceInput = document.getElementById('sentenceInput');
        const encodeButton = document.getElementById('encodeButton');
        const encodedResultsDiv = document.getElementById('encodedResults');
        const wordEncodingsDiv = document.getElementById('wordEncodings');
        const sentenceEmbeddingDiv = document.getElementById('sentenceEmbedding');
        const messageBox = document.getElementById('messageBox');

        // New elements for conceptual core transformation output
        const tanWebApiReduceDiv = document.getElementById('tanWebApiReduce');
        const sinBibleCosPMDiv = document.getElementById('sinBibleCosPM');
        const tanIncidenceWebApiDiv = document.getElementById('tanIncidenceWebApi');


        // Function to display messages
        function showMessage(message, type = 'info') {
            messageBox.textContent = message;
            messageBox.classList.remove('hidden', 'bg-red-500', 'bg-green-500', 'bg-blue-500');
            if (type === 'error') {
                messageBox.classList.add('bg-red-500');
            } else if (type === 'success') {
                messageBox.classList.add('bg-green-500');
            } else {
                messageBox.classList.add('bg-blue-500');
            }
            messageBox.classList.add('block');
        }

        encodeButton.addEventListener('click', async () => {
            const sentence = sentenceInput.value.trim();
            if (!sentence) {
                showMessage('Please enter a sentence to encode.', 'error');
                return;
            }

            showMessage('Encoding sentence using Merkaba-Native USE...', 'info');
            wordEncodingsDiv.innerHTML = ''; // Clear previous results
            encodedResultsDiv.classList.remove('hidden');

            // Simple tokenization: split by spaces and remove punctuation
            const words = sentence.toLowerCase().split(/\s+/).map(word => word.replace(/[.,!?;:"]/g, '')).filter(word => word.length > 0);
            let globalClock = 0; // Represents a conceptual global clock for this sentence's processing

            const wordSemanticEmbeddings = [];

            for (const word of words) {
                globalClock++;
                const encoded = await encodeWordEntity(word, globalClock, globalClock);

                wordSemanticEmbeddings.push(encoded.semanticEmbedding);

                const card = document.createElement('div');
                card.className = 'encoding-card';
                card.innerHTML = `
                    <p class="text-xl font-bold text-yellow-300 mb-2">Word: "${encoded.word}"</p>
                    <p><strong class="text-indigo-200">Conceptual N-Value (Dimensionality):</strong> <span class="font-mono text-green-300">${encoded.n_value}D</span></p>
                    <p><strong class="text-indigo-200">Pascal Values [0,1,2]:</strong> <span class="font-mono text-green-300">[${encoded.pascal.join(', ')}]</span></p>
                    <p><strong class="text-indigo-200">Conceptual Vector Clock:</strong> <span class="font-mono text-green-300">${encoded.conceptualVectorClock}</span></p>
                    <p><strong class="text-indigo-200">Semantic Embedding Component (tan lambda):</strong> <span class="font-mono text-green-300">${encoded.semanticEmbedding.toFixed(6)}</span></p>
                    <p><strong class="text-indigo-200">Conceptual Ethers Address:</strong> <span class="font-mono text-sm text-gray-400 break-words">${encoded.ethersAddress}</span></p>
                    <p><strong class="text-indigo-200">Conceptual Binary ArrayBuffer (Float32):</strong> <span class="font-mono text-sm text-gray-400 break-words">[${Array.from(encoded.arrayBuffer).map(val => val.toFixed(4)).join(', ')}]</span></p>
                `;
                wordEncodingsDiv.appendChild(card);
            }

            let overallSentenceEmbedding = 0;
            if (wordSemanticEmbeddings.length > 0) {
                overallSentenceEmbedding = wordSemanticEmbeddings.reduce((sum, val) => sum + val, 0) / wordSemanticEmbeddings.length;
            }
            sentenceEmbeddingDiv.textContent = overallSentenceEmbedding.toFixed(6);

            // --- Conceptual Core Transformation Execution ---
            // For demonstration, we'll use aggregated properties from the input sentence
            // to represent the 'reduce' functions for WebAPI, Bible, and Principia Mathematica.
            // These are highly abstract conceptual values here.

            // Conceptual WebApi.reduce: related to sentence length/complexity, representing external data flow
            const conceptualWebApiReduce = sentence.length + words.length;
            // Conceptual Bible.reduce: related to cumulative word values, representing semantic feature set
            const conceptualBibleReduce = words.reduce((sum, word) => sum + word.charCodeAt(0), 0) % 360; // Modulo 360 for angle
            // Conceptual PrincipiaMathematica.reduce: related to structured aspects, perhaps sum of pascal 2-indices
            const conceptualPrincipiaMathematicaReduce = words.reduce((sum, word) => sum + pascalValue(Math.min(50, Math.max(0, word.length + (word.split('').reduce((a,b)=>a+b.charCodeAt(0),0)%10))), 2), 0) % 360; // Modulo 360

            // Conceptual WebApi.incidence.reduce: related to the interplay/relationships in the external data
            const conceptualWebApiIncidenceReduce = (conceptualWebApiReduce * 0.7) % 360; // Arbitrary derivative for conceptual incidence

            const coreTransformResults = conceptualCoreTransformation(
                conceptualWebApiReduce,
                conceptualBibleReduce,
                conceptualPrincipiaMathematicaReduce,
                conceptualWebApiIncidenceReduce
            );

            tanWebApiReduceDiv.textContent = coreTransformResults.tanWebApiReduce.toFixed(6);
            sinBibleCosPMDiv.textContent = coreTransformResults.sinBibleCosPM.toFixed(6);
            tanIncidenceWebApiDiv.textContent = coreTransformResults.tanIncidenceWebApi.toFixed(6);


            showMessage('Sentence encoded successfully into conceptual ArrayBuffers and Core Transformation applied!', 'success');
        });

        // Initial encoding on load with default sentence
        window.onload = () => {
            encodeButton.click();
        };

    </script>
</body>
</html>
